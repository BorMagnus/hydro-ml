{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if module_path not in sys.path:\n",
    "    os.environ[\"PYTHONPATH\"] = module_path\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.train import train_model\n",
    "from src.experiment import get_variables_combinations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Tuning a Machine Learning Model with Population-Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    i,\n",
    "    model,\n",
    "    exp_name,\n",
    "    file_name,\n",
    "    n_samples,\n",
    "    max_num_epochs,\n",
    "    min_num_epochs,\n",
    "    local_dir=\"../ray_results\",\n",
    "):\n",
    "    target_variable = \"Flow_Kalltveit\"\n",
    "    datetime_variable = \"Datetime\"\n",
    "\n",
    "    variables = [get_variables_combinations(file_name, datetime_variable)[i]]\n",
    "\n",
    "    config = {\n",
    "        \"data_file\": file_name,\n",
    "        \"datetime\": datetime_variable,\n",
    "        \"data\": {\n",
    "            \"target_variable\": target_variable,\n",
    "            \"sequence_length\": tune.choice([25]),\n",
    "            \"batch_size\": tune.choice([256]),\n",
    "            \"variables\": tune.grid_search(variables),\n",
    "            \"split_size\": {\"train_size\": 0.7, \"val_size\": 0.2, \"test_size\": 0.1},\n",
    "        },\n",
    "        \"model\": tune.grid_search(model),\n",
    "        \"model_arch\": {\n",
    "            \"input_size\": tune.sample_from(\n",
    "                lambda spec: len(spec.config.data[\"variables\"]) + 1\n",
    "            ),\n",
    "            \"hidden_size\": tune.choice([32, 64]),\n",
    "            \"num_layers\": tune.choice([1, 2, 3]),\n",
    "            \"output_size\": 1,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"learning_rate\": tune.loguniform(1e-5, 1e-1),\n",
    "            \"weight_decay\": tune.loguniform(1e-5, 1e-1),\n",
    "        },\n",
    "        \"num_epochs\": max_num_epochs,\n",
    "    }\n",
    "\n",
    "    reporter = tune.JupyterNotebookReporter(\n",
    "        metric_columns=[\"train_loss\", \"val_loss\", \"test_loss\", \"training_iteration\"]\n",
    "    )\n",
    "\n",
    "    scheduler_population = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        perturbation_interval=min_num_epochs,\n",
    "        hyperparam_mutations={\n",
    "            \"weight_decay\": tune.uniform(0.0, 0.3),\n",
    "            \"learning_rate\": tune.loguniform(1e-5, 1e-1),\n",
    "            \"model_arch.hidden_size\": tune.choice([32, 64]),\n",
    "            \"model_arch.num_layers\": tune.choice([1, 2, 3]),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": max_num_epochs,\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    results = tune.run(\n",
    "        train_model,\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=n_samples,\n",
    "        scheduler=scheduler_population,\n",
    "        progress_reporter=reporter,\n",
    "        name=exp_name,\n",
    "        local_dir=local_dir,\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        stop=stop,\n",
    "        # search_alg=search_alg, # Add the chosen search algorithm\n",
    "        keep_checkpoints_num=1,\n",
    "        checkpoint_score_attr=\"val_loss\",\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Multiple Machine Learning Models on Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "clean_data_dir = os.path.abspath(os.path.join(data_dir, \"clean_data\"))\n",
    "\n",
    "results = []\n",
    "\n",
    "model_dict = {\n",
    "    \"test-lstm\": \"LSTM\",\n",
    "    \"test-temp\": \"LSTMTemporalAttention\",\n",
    "    \"test-spa_temp\": \"LSTMSpatioTemporalAttention\",\n",
    "    \"test-fcn\": \"FCN\",\n",
    "}\n",
    "for i in range(4):\n",
    "    for exp_name, model in model_dict.items():\n",
    "        filename = \"cleaned_data_4.csv\"\n",
    "        # Get the full path of the file\n",
    "        file_path = os.path.join(clean_data_dir, filename)\n",
    "\n",
    "        num = filename.split(\"_\")[2].split(\".\")[0]\n",
    "        experiment = f\"data_{num}-{exp_name}\"\n",
    "\n",
    "        analysis = main(\n",
    "            i,\n",
    "            [model],\n",
    "            exp_name=experiment,\n",
    "            file_name=filename,\n",
    "            n_samples=25,\n",
    "            max_num_epochs=100,\n",
    "            min_num_epochs=25,\n",
    "        )\n",
    "\n",
    "        results.append(analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining and Displaying Results of Machine Learning Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []  # List to store results\n",
    "\n",
    "for analysis in results:\n",
    "    df = analysis.dataframe()[\n",
    "        [\n",
    "            \"train_loss\",\n",
    "            \"val_loss\",\n",
    "            \"train_loss\",\n",
    "            \"config/model\",\n",
    "            \"time_total_s\",\n",
    "            \"config/data/variables\",\n",
    "        ]\n",
    "    ]\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all results into a single dataframe\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(combined_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydro-ml",
   "language": "python",
   "name": "hydro-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
