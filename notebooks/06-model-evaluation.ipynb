{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.data import *\n",
    "from src.train import create_model\n",
    "from src.evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"test\"\n",
    "best = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all model directories\n",
    "ray_results = Path(\"../ray_results/\")\n",
    "model_dirs = [d for d in ray_results.iterdir() if d.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_models(model_dirs, experiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data points and time series region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs = {}\n",
    "parameters = []\n",
    "for model_dir in model_dirs:\n",
    "    if experiment not in str(model_dir):\n",
    "        continue\n",
    "    rows = []\n",
    "    best_checkpoints = find_best_checkpoints(model_dir, num_best=best)\n",
    "    for i, (checkpoint, val_loss, params) in enumerate(best_checkpoints):\n",
    "        # Load model and weights\n",
    "        model = create_model(params)\n",
    "        model = load_model_from_checkpoint(model, checkpoint)\n",
    "\n",
    "        data_loader, scalers = get_dataloader(params)\n",
    "        test_loader = data_loader[\"test\"]\n",
    "        test_loader_length = len(test_loader.dataset)\n",
    "        print(\"Number of values in test_loader:\", test_loader_length)\n",
    "        datetime_test = test_loader.datetime_index\n",
    "        time_series_region = (datetime_test[0], datetime_test[-1])\n",
    "        print(\"Time series region:\", time_series_region)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir in model_dirs:\n",
    "    if experiment not in str(model_dir):\n",
    "        continue\n",
    "    best_checkpoints = find_best_checkpoints(model_dir, num_best=1)\n",
    "    for i, (checkpoint, val_loss, params) in enumerate(best_checkpoints):\n",
    "        print(f\"Best model from {model_dir}\")\n",
    "        run_dir = checkpoint.parents[1]\n",
    "        losses = get_losses(run_dir)\n",
    "        plot_losses(losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour ahead forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_actual(model_dirs, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs, parameters = calculate_model_metrics(model_dirs, experiment, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame\n",
    "df = pd.DataFrame(parameters)\n",
    "df.sort_values(\"val_mea\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes\n",
    "df_concat_avg = pd.concat([model_dfs[k] for k in model_dfs.keys() if experiment in k])\n",
    "\n",
    "df_concat_avg = df_concat_avg.drop(columns=[\"variables\"])\n",
    "\n",
    "# calculate the mean of each evaluation metric\n",
    "df_avg = df_concat_avg.groupby([\"model\"]).mean()\n",
    "df_avg.sort_values(\"test_mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_avg_w_var = average_with_var(model_dfs, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_statistics(df_concat_avg_w_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot(df_concat_avg_w_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_iqr(df_concat_avg_w_var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-space consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def find_best_checkpoints_with_time(model_dir, num_best=5):\n",
    "    checkpoints = []\n",
    "\n",
    "    # Iterate over all training runs in the model directory\n",
    "    for run_dir in model_dir.iterdir():\n",
    "        if run_dir.is_dir():\n",
    "            # Read the progress.csv file to get the validation losses and training time\n",
    "            progress_file = run_dir / \"progress.csv\"\n",
    "            if progress_file.exists():\n",
    "                with open(run_dir / \"params.json\", \"r\") as f:\n",
    "                    params = json.load(f)\n",
    "                progress_data = pd.read_csv(progress_file)\n",
    "\n",
    "                best_val_idx = progress_data[\"val_loss\"].idxmin()\n",
    "                best_val_loss = progress_data.loc[best_val_idx, \"val_loss\"]\n",
    "                training_time = progress_data.loc[best_val_idx, \"time_total_s\"]\n",
    "\n",
    "                # Save the checkpoint path, validation loss, and training time\n",
    "                checkpoint_path = run_dir / \"my_model\" / \"checkpoint.pt\"\n",
    "                checkpoints.append(\n",
    "                    (checkpoint_path, best_val_loss, training_time, params)\n",
    "                )\n",
    "\n",
    "    # Sort the checkpoints based on validation loss\n",
    "    checkpoints.sort(key=itemgetter(1))\n",
    "\n",
    "    return checkpoints[:num_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    if experiment not in str(model_dir):\n",
    "        continue\n",
    "\n",
    "    best_checkpoints = find_best_checkpoints_with_time(model_dir, num_best=best)\n",
    "\n",
    "    for i, (checkpoint, val_loss, training_time, params) in enumerate(best_checkpoints):\n",
    "        # Load model and weights\n",
    "        model = create_model(params)\n",
    "\n",
    "        # Calculate space consumption\n",
    "        num_parameters = sum(p.numel() for p in model.parameters())\n",
    "        space_consumption = num_parameters * 4  # 4 bytes per parameter (float32)\n",
    "\n",
    "        model_name = params[\"model\"]\n",
    "        if model_name not in results:\n",
    "            results[model_name] = {\n",
    "                \"val_loss\": [],\n",
    "                \"training_time\": [],\n",
    "                \"testing_time\": [],\n",
    "                \"space_consumption\": [],\n",
    "            }\n",
    "\n",
    "        results[model_name][\"val_loss\"].append(val_loss)\n",
    "        results[model_name][\"training_time\"].append(training_time)\n",
    "        results[model_name][\"space_consumption\"].append(space_consumption)\n",
    "\n",
    "        # Add testing time to results\n",
    "        testing_time = np.mean(model_dfs[model_dir.name][\"testing (s)\"])\n",
    "        results[model_name][\"testing_time\"].append(testing_time)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Avg val_loss\",\n",
    "        \"Avg training_time\",\n",
    "        \"Avg testing_time\",\n",
    "        \"Avg space_consumption\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for model_name, data in results.items():\n",
    "    avg_val_loss = np.mean(data[\"val_loss\"])\n",
    "    avg_training_time = np.mean(data[\"training_time\"])\n",
    "    avg_testing_time = np.mean(data[\"testing_time\"])\n",
    "    avg_space_consumption = (\n",
    "        np.mean(data[\"space_consumption\"]) / 1024\n",
    "    )  # Space consumption in KB\n",
    "\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Model\": [model_name],\n",
    "                    \"Avg val_loss\": [avg_val_loss],\n",
    "                    \"Avg training_time\": [avg_training_time],\n",
    "                    \"Avg testing_time\": [avg_testing_time],\n",
    "                    \"Avg space_consumption\": [\n",
    "                        avg_space_consumption\n",
    "                    ],  # Space consumption in KB\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df.sort_values(\"Avg training_time\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention weights understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def visualize_attention(\n",
    "    attention_weights_spatial, attention_weights_temporal, batch_idx, features\n",
    "):\n",
    "    # Extract attention weights for a specific batch element\n",
    "    attention_matrix_spatial = (\n",
    "        attention_weights_spatial[batch_idx].detach().cpu().numpy()\n",
    "    )\n",
    "    attention_matrix_temporal = (\n",
    "        attention_weights_temporal[batch_idx].detach().cpu().numpy()\n",
    "    )\n",
    "    zmax_spatial1 = np.percentile(attention_matrix_spatial, 95)\n",
    "    zmax_temporal = np.percentile(attention_matrix_temporal, 95)\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=2,\n",
    "        subplot_titles=(\"Spatial Attention Weights\", \"Temporal Attention Weights\"),\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "\n",
    "    # Add spatial attention heatmap to subplot\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=attention_matrix_spatial,\n",
    "            x=[f\"f{i}\" for i in range(1, attention_matrix_spatial.shape[1] + 1)],\n",
    "            y=[f\"t-{i}\" for i in range(1, attention_matrix_spatial.shape[0] + 1)],\n",
    "            colorscale=\"Viridis\",\n",
    "            zmin=0,\n",
    "            zmax=zmax_spatial1,\n",
    "            name=\"Spatial Weights\",\n",
    "            colorbar_x=0.45\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # Add temporal attention heatmap to subplot\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=attention_matrix_temporal,\n",
    "            y=[f\"t-{i}\" for i in range(1, attention_matrix_temporal.shape[0] + 1)],\n",
    "            x=[f\"f{i}\" for i in range(1, attention_matrix_temporal.shape[1] + 1)],\n",
    "            colorscale=\"Viridis\",\n",
    "            zmin=0,\n",
    "            zmax=zmax_temporal,\n",
    "            name=\"Temporal Weights\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1800,\n",
    "        height=800,\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Input Time Step\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_attention(params, spatial_weights=None, temporal_weights=None):\n",
    "    features = [params[\"data\"][\"target_variable\"]] + params[\"data\"][\"variables\"]\n",
    "    b = 20\n",
    "    # If the attention weights are torch tensors, convert them to numpy arrays first\n",
    "    if isinstance(spatial_weights, torch.Tensor) and isinstance(\n",
    "        temporal_weights, torch.Tensor\n",
    "    ):\n",
    "        visualize_attention(\n",
    "            spatial_weights, temporal_weights, batch_idx=b, features=features\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir in model_dirs:\n",
    "    #if experiment not in str(model_dir):\n",
    "    #    continue\n",
    "    rows = []\n",
    "    best_checkpoints = find_best_checkpoints(model_dir, num_best=1)\n",
    "    for i, (checkpoint, val_loss, params) in enumerate(best_checkpoints):\n",
    "        if params[\"model\"] == \"LSTM\" or params[\"model\"] == \"FCN\":\n",
    "            continue\n",
    "        elif params[\"model\"] == \"LSTMTemporalAttention\":\n",
    "            continue\n",
    "\n",
    "        # Load model and weights\n",
    "        model = create_model(params)\n",
    "        model = load_model_from_checkpoint(model, checkpoint)\n",
    "        data_loader, _ = get_dataloader(params)\n",
    "        test_dataloader = data_loader[\"test\"]\n",
    "\n",
    "        # Get a batch of input sequences and their corresponding targets\n",
    "        inputs, targets = next(iter(test_dataloader))\n",
    "\n",
    "        output, spatial_attention_weights, temporal_attention_weights = model(\n",
    "            inputs, True\n",
    "        )\n",
    "\n",
    "        plot_attention(params, \n",
    "                       spatial_attention_weights, \n",
    "                       temporal_attention_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-time step ahead forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_ahead = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs = evaluate_multi_step_models(model_dirs, experiment, steps_ahead, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs.get(f\"data_4-{experiment}-lstm\").sort_values(\"test_mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs.get(f\"data_4-{experiment}-temp\").sort_values(\"test_mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dfs.get(f\"data_4-{experiment}-spa_temp\").sort_values(\"test_mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes\n",
    "df_concat_avg = pd.concat([model_dfs[k] for k in model_dfs.keys() if experiment in k])\n",
    "\n",
    "df_concat_avg = df_concat_avg.drop(columns=[\"variables\"])\n",
    "\n",
    "# calculate the mean of each evaluation metric\n",
    "df_avg = df_concat_avg.groupby([\"model\"]).mean()\n",
    "df_avg.sort_values(\"test_mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_avg_w_var = average_with_var(model_dfs, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes\n",
    "descriptive_statistics(df_concat_avg_w_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot(df_concat_avg_w_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_iqr(df_concat_avg_w_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention_dual(\n",
    "    attention_weights_spatial1, attention_weights_temporal1,\n",
    "    attention_weights_spatial2, attention_weights_temporal2, \n",
    "    batch_idx, features\n",
    "):\n",
    "    # Extract attention weights for a specific batch element\n",
    "    attention_matrix_spatial1 = attention_weights_spatial1[batch_idx].detach().cpu().numpy()\n",
    "    attention_matrix_temporal1 = attention_weights_temporal1[batch_idx].detach().cpu().numpy()\n",
    "\n",
    "    attention_matrix_spatial2 = attention_weights_spatial2[batch_idx].detach().cpu().numpy()\n",
    "    attention_matrix_temporal2 = attention_weights_temporal2[batch_idx].detach().cpu().numpy()\n",
    "\n",
    "    # Calculate 95th percentile of the attention weights\n",
    "    zmax_spatial1 = np.percentile(attention_matrix_spatial1, 95)\n",
    "    zmax_temporal1 = np.percentile(attention_matrix_temporal1, 95)\n",
    "    zmax_spatial2 = np.percentile(attention_matrix_spatial2, 95)\n",
    "    zmax_temporal2 = np.percentile(attention_matrix_temporal2, 95)\n",
    "\n",
    "    # Create a subplot with 1 row and 4 columns\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=4,\n",
    "        subplot_titles=(\"Spatial Attention Weights - 1\", \"Temporal Attention Weights - 1\",\n",
    "                        \"Spatial Attention Weights - 12\", \"Temporal Attention Weights - 12\"),\n",
    "        horizontal_spacing=0.10\n",
    "    )\n",
    "\n",
    "    # Add spatial attention heatmap for step 1\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=attention_matrix_spatial1,\n",
    "            \n",
    "            x=features,\n",
    "            y=[f\"t-{i}\" for i in range(1, attention_matrix_spatial1.shape[0] + 1)],\n",
    "            colorscale=\"Viridis\",\n",
    "            zmin=0,\n",
    "            zmax=zmax_spatial1,\n",
    "            name=\"Spatial Weights - 1\", colorbar_x=0.20\n",
    "        ),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "\n",
    "    # Add temporal attention heatmap for step 1\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=attention_matrix_temporal1,\n",
    "            y=[f\"t-{i}\" for i in range(1, attention_matrix_temporal1.shape[0] + 1)],\n",
    "            x=features,\n",
    "            colorscale=\"Viridis\",\n",
    "            zmin=0,\n",
    "            zmax=zmax_temporal1,\n",
    "            name=\"Temporal Weights - 1\", colorbar_x=0.48\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Add spatial attention heatmap for step 12 \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=attention_matrix_spatial2,\n",
    "            x=features,\n",
    "            y=[f\"t-{i}\" for i in range(1, attention_matrix_spatial2.shape[0] + 1)],\n",
    "            colorscale=\"Viridis\",\n",
    "            zmin=0,\n",
    "            zmax=zmax_spatial2,\n",
    "            name=\"Spatial Weights - 12\", colorbar_x=0.75\n",
    "        ),\n",
    "        row=1, col=3,\n",
    "    )\n",
    "\n",
    "    # Add temporal attention heatmap for step 12\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=attention_matrix_temporal2,\n",
    "            y=[f\"t-{i}\" for i in range(1, attention_matrix_temporal2.shape[0] + 1)],\n",
    "            x=features,\n",
    "            colorscale=\"Viridis\",\n",
    "            zmin=0,\n",
    "            zmax=zmax_temporal2,\n",
    "            name=\"Temporal Weights - 12\", colorbar_x=1.02\n",
    "        ),\n",
    "        row=1, col=4\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1800,\n",
    "        height=800,\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Input Time Step\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def plot_attention_dual(params, spatial_weights1=None, temporal_weights1=None, spatial_weights2=None, temporal_weights2=None):\n",
    "    features = [params[\"data\"][\"target_variable\"]] + params[\"data\"][\"variables\"]\n",
    "    b = 7\n",
    "    visualize_attention_dual(\n",
    "        spatial_weights1, temporal_weights1, spatial_weights2, temporal_weights2, batch_idx=b, features=features\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_multi_step_models(model_dirs, experiment, steps_ahead, best):\n",
    "    model_dfs = {}\n",
    "    for model_dir in model_dirs:\n",
    "        if experiment not in str(model_dir):\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        best_checkpoints = find_best_checkpoints(model_dir, num_best=best)\n",
    "        for i, (checkpoint, val_loss, params) in enumerate(best_checkpoints):\n",
    "            if params[\"model\"] == \"LSTM\" or params[\"model\"] == \"FCN\":\n",
    "                continue\n",
    "            elif params[\"model\"] == \"LSTMTemporalAttention\":\n",
    "                continue\n",
    "\n",
    "            # Load model and weights\n",
    "            model = create_model(params)\n",
    "            model = load_model_from_checkpoint(model, checkpoint)\n",
    "\n",
    "            data_loader, scalers = get_dataloader(params)\n",
    "            test_loader = data_loader[\"test\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_preds, y_test, attention_weights_all = get_multi_step_preds_actuals(\n",
    "                    model, test_loader, forecast_steps=steps_ahead, return_weights=True\n",
    "                )\n",
    "            attention_weights_1, attention_weights_12 = attention_weights_all[7]\n",
    "            alpha_list_1, beta_t_1 = attention_weights_1\n",
    "            alpha_list_12, beta_t_12 = attention_weights_12\n",
    "\n",
    "            plot_attention_dual(params, spatial_weights1=alpha_list_1, temporal_weights1=beta_t_1, spatial_weights2=alpha_list_12, temporal_weights2=beta_t_12)\n",
    "\n",
    "plot_attention_multi_step_models(model_dirs, experiment, steps_ahead, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydro-ml",
   "language": "python",
   "name": "hydro-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
