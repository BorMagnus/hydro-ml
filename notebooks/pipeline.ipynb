{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse_sklearn\n",
    "from sklearn.metrics import mean_absolute_error as mae_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_GPU = torch.cuda.is_available()\n",
    "if use_GPU:\n",
    "    mode = {\"name\": \"cuda\", \"device\": torch.device(\"cuda\")}\n",
    "else:\n",
    "    mode = {\"name\": \"cpu\", \"device\": torch.device(\"cpu\")}\n",
    "\n",
    "in_dim = 1\n",
    "hidden_dim = 64\n",
    "out_dim = 1\n",
    "sequence_length = 72\n",
    "batch_size = 128 # 32, 64, 128, 256\n",
    "\n",
    "num_epochs=10\n",
    "num_workers=11\n",
    "lr = 1e-3\n",
    "regularization=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    data = []\n",
    "    for i in [\"/train\", \"/val\", \"/test\"]:\n",
    "        data.append(pd.read_csv(path + i + \".csv\", index_col=\"Datetime\"))\n",
    "    return data[0], data[1], data[2]\n",
    "# Load data\n",
    "path = '../data/clean_data/univariate_Q_Kalltveit'\n",
    "train, val, test = get_data(path)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing preprocessing part\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "  \n",
    "train_transformed = sc.fit_transform(train)\n",
    "val_transformed = sc.transform(val)\n",
    "test_transformed = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = sliding_windows(train_transformed, sequence_length)\n",
    "X_val, y_val = sliding_windows(val_transformed, sequence_length)\n",
    "X_test, y_test = sliding_windows(test_transformed, sequence_length)\n",
    "\n",
    "X_train, X_val, X_test = X_train.reshape(len(X_train), -1), X_val.reshape(len(X_val), -1), X_test.reshape(len(X_test), -1)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=(0,0,0),std=(1,1,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Applying PCA function on training\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "sequence_length = 5\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class data_trans(Dataset):\n",
    "        \n",
    "    def __init__(self, data, groundtruth, transform=None, pca=None):\n",
    "\n",
    "        self.data = self._get_data(data)\n",
    "        self.groundtruth = self._get_data(groundtruth)\n",
    "        self.transform = transform\n",
    "        self.pca\n",
    "\n",
    "    def _get_data(self,data):\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "               \n",
    "        inputs = self.data[index,:]\n",
    "        groundtruths = self.groundtruth[index,:]\n",
    "        \n",
    "        if self.transform:\n",
    "                        \n",
    "            inputs = torch.from_numpy(inputs).float()\n",
    "            groundtruths = torch.from_numpy(groundtruths).float()\n",
    "\n",
    "        return [inputs, groundtruths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_trans = data_trans(X_train, y_train, transform)\n",
    "val_data_trans = data_trans(X_val, y_val, transform)\n",
    "test_data_trans = data_trans(X_test, y_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data_trans,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = False,\n",
    "                                           num_workers = 0)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data_trans,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = False,\n",
    "                                           num_workers = 0)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_trans,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = False,\n",
    "                                           num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"val\": val_dataloader,\n",
    "    \"test\": test_dataloader,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, sequence_length, mode):\n",
    "\n",
    "        super(FCN, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.mode = mode\n",
    "\n",
    "        self.layer_in = nn.Linear(sequence_length, hidden_dim, bias=False)\n",
    "        self.fcn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_out = nn.Linear(hidden_dim, out_dim, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward (self,input):\n",
    "        out = self.layer_in(input)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.fcn(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.layer_out(out)\n",
    "      \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, sequence_length, mode):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.mode = mode\n",
    "        \n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = nn.LSTMCell(in_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_out = nn.Linear(hidden_dim*sequence_length, out_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, y):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32).to(self.mode[\"device\"])\n",
    "        c_t = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32).to(self.mode[\"device\"])\n",
    "        h_t2 = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32).to(self.mode[\"device\"])\n",
    "        c_t2 = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32).to(self.mode[\"device\"])\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            h_t, c_t = self.lstm1(time_step, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        out = self.linear_out(outputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, sequence_length, mode):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.mode = mode\n",
    "\n",
    "        self.lstm1 = nn.LSTMCell(in_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.T_A = nn.Linear(sequence_length*hidden_dim, sequence_length)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, y):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(y.size(0), self.hidden_dim).to(self.mode[\"device\"])\n",
    "        c_t = torch.zeros(y.size(0), self.hidden_dim).to(self.mode[\"device\"])\n",
    "        h_t2 = torch.zeros(y.size(0), self.hidden_dim).to(self.mode[\"device\"])\n",
    "        c_t2 = torch.zeros(y.size(0), self.hidden_dim).to(self.mode[\"device\"])\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(time_step, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "            \n",
    "        total_ht = outputs[0]\n",
    "        for i in range(1, len(outputs)):\n",
    "            total_ht = torch.cat((total_ht, outputs[i]), 1)\n",
    "\n",
    "        beta_t =  self.relu(self.T_A(total_ht))\n",
    "        beta_t = self.softmax(beta_t)\n",
    "\n",
    "        out = torch.zeros(y.size(0), self.hidden_dim).to(self.mode[\"device\"])\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "                      \n",
    "            out = out + outputs[i]*beta_t[:,i].reshape(out.size(0), 1)\n",
    "\n",
    "        out = self.linear_out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import mean_absolute_error\n",
    "\n",
    "def fit(model, loss_function, optimizer, data_loader, num_epochs, mode, use_amp=False):\n",
    "\thistory = {\"train\": {\"loss\": [], \"mae\": []}, \"val\": {\"loss\": [], \"mae\": []}}\n",
    "\tscaler = torch.cuda.amp.GradScaler(enabled=use_amp) # Mixed-precision support for compatible GPUs\n",
    "\tprint(\"\\nTraining the model:\")\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint(\"\\nEpoch\", epoch+1)\n",
    "\t\tif epoch < num_epochs - 1:\n",
    "\t\t\tkeys = [\"train\", \"val\"]\n",
    "\t\telse:\n",
    "\t\t\tkeys = [\"train\", \"val\", \"test\"]\n",
    "\t\tfor key in keys:\n",
    "\t\t\tdataset_size = 0\n",
    "\t\t\tdataset_loss = 0.0\n",
    "\t\t\t#dataset_mae = []\n",
    "\t\t\tif key == \"train\":\n",
    "\t\t\t\tmodel.train()\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.eval()\n",
    "\t\t\tfor X_batch, y_batch in tqdm(data_loader[key]):\n",
    "\t\t\t\tX_batch, y_batch = X_batch.to(mode[\"device\"]), y_batch.to(mode[\"device\"])\n",
    "\t\t\t\twith torch.set_grad_enabled(mode=(key==\"train\")): # Autograd activated only during training\n",
    "\t\t\t\t\twith torch.cuda.amp.autocast(enabled=use_amp): # Mixed-precision support for compatible GPUs\n",
    "\t\t\t\t\t\tbatch_output = model(X_batch.float())\n",
    "\t\t\t\t\t\tbatch_loss = loss_function(batch_output, y_batch)\n",
    "\t\t\t\t\tif key == \"train\":\n",
    "\t\t\t\t\t\tscaler.scale(batch_loss).backward()\n",
    "\t\t\t\t\t\tscaler.step(optimizer) \t\n",
    "\t\t\t\t\t\tscaler.update()\n",
    "\t\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tdataset_size += y_batch.shape[0]\n",
    "\t\t\t\tdataset_loss += y_batch.shape[0] * batch_loss.item()\n",
    "\t\t\tdataset_loss /= dataset_size\n",
    "\t\t\tif key in [\"train\", \"val\"]:\n",
    "\t\t\t\thistory[key][\"loss\"].append(dataset_loss)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"\\nEvaluating the model:\")\n",
    "\t\t\tprint(key, \"loss:\", dataset_loss)\n",
    "\treturn history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = FCN(in_dim, hidden_dim, out_dim, sequence_length).to(mode[\"device\"])\n",
    "optimizer = optimizer = optim.Adam(model_1.parameters(), lr=lr, weight_decay=regularization)\n",
    "error_criterion = nn.MSELoss().to(mode[\"device\"])\n",
    "\n",
    "history = fit(model=model_1, loss_function=error_criterion, optimizer=optimizer, data_loader=data_loader, num_epochs=num_epochs, mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history(history):\n",
    "\tabsciss = np.arange(1, len(history[\"train\"][\"loss\"])+1)\n",
    "\tplt.figure()\n",
    "\tplt.suptitle(\"Training history\")\n",
    "\tplt.subplot(121)\n",
    "\tplt.title(\"Loss history\")\n",
    "\tplt.plot(absciss, history[\"train\"][\"loss\"], label=\"Train\")\n",
    "\tplt.plot(absciss, history[\"val\"][\"loss\"], label=\"Validation\")\n",
    "\tplt.xlabel(\"Epoch\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "print_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LSTM(in_dim, hidden_dim, out_dim, sequence_length)\n",
    "model_2.to(mode[\"device\"])\n",
    "optimizer = optimizer = optim.Adam(model_2.parameters(), lr=lr, weight_decay=regularization)\n",
    "error_criterion = nn.MSELoss().to(mode[\"device\"])\n",
    "history = fit(model=model_2, \n",
    "            loss_function=error_criterion, \n",
    "            optimizer=optimizer, \n",
    "            data_loader=data_loader, \n",
    "            num_epochs=num_epochs, \n",
    "            mode=mode\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history(history):\n",
    "\tabsciss = np.arange(1, len(history[\"train\"][\"loss\"])+1)\n",
    "\tplt.figure()\n",
    "\tplt.suptitle(\"Training history\")\n",
    "\tplt.subplot(121)\n",
    "\tplt.title(\"Loss history\")\n",
    "\tplt.plot(absciss, history[\"train\"][\"loss\"], label=\"Train\")\n",
    "\tplt.plot(absciss, history[\"val\"][\"loss\"], label=\"Validation\")\n",
    "\tplt.xlabel(\"Epoch\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "print_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = LSTM(in_dim, hidden_dim, out_dim, sequence_length).to(mode[\"device\"])\n",
    "optimizer = optimizer = optim.Adam(model_3.parameters(), lr=lr, weight_decay=regularization)\n",
    "error_criterion = nn.MSELoss().to(mode[\"device\"])\n",
    "history = fit(model=model_3, loss_function=error_criterion, optimizer=optimizer, data_loader=data_loader, num_epochs=num_epochs, mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history(history):\n",
    "\tabsciss = np.arange(1, len(history[\"train\"][\"loss\"])+1)\n",
    "\tplt.figure()\n",
    "\tplt.suptitle(\"Training history\")\n",
    "\tplt.subplot(121)\n",
    "\tplt.title(\"Loss history\")\n",
    "\tplt.plot(absciss, history[\"train\"][\"loss\"], label=\"Train\")\n",
    "\tplt.plot(absciss, history[\"val\"][\"loss\"], label=\"Validation\")\n",
    "\tplt.xlabel(\"Epoch\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "print_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader, model_name):\n",
    "    preds = []\n",
    "    ground_truth = []\n",
    "    model.eval()\n",
    "    df = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            y_pred = model(X.to(mode[\"device\"]))\n",
    "            preds.append([tensor.item() for tensor in y_pred])\n",
    "            ground_truth.append([tensor.item() for tensor in y])\n",
    "    preds = [item for sublist in preds for item in sublist]\n",
    "    ground_truth = [item for sublist in ground_truth for item in sublist]\n",
    "\n",
    "    df[\"pred\"] = preds\n",
    "    df[\"ground_truth\"] = ground_truth\n",
    "    df.index = test_index[73:]\n",
    "\n",
    "    mae = mae_sklearn(preds, ground_truth)\n",
    "    mse = mse_sklearn(preds, ground_truth, squared=True)\n",
    "    rmse = mse_sklearn(preds, ground_truth, squared=False)\n",
    "\n",
    "    print(f\"Performance measuers with unseen data on {model_name}\")\n",
    "    print(\"MAE: {:.2f}\".format(mae), \"MSE: {:.2f}\".format(mse), \"RMSE: {:.2f}\".format(rmse))\n",
    "    print()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_history = predict(model_1, test_dataloader, \"FCN\")\n",
    "model_2_history = predict(model_2, test_dataloader, \"LSTM\")\n",
    "model_3_history = predict(model_3, test_dataloader, \"LSTM Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n",
    "model_3_history.iplot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab06ce908a8abf9762b166587a79a7b3fe0760e63d13e53395d21ef1a2a21042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
