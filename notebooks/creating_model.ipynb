{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 24]) torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "use_GPU = torch.cuda.is_available()\n",
    "if use_GPU:\n",
    "    mode = {\"name\": \"cuda\", \"device\": torch.device(\"cuda\")}\n",
    "else:\n",
    "    mode = {\"name\": \"cpu\", \"device\": torch.device(\"cpu\")}\n",
    "error_criterion = nn.MSELoss().to(mode[\"device\"])\n",
    "loss_criterion = nn.MSELoss().to(mode[\"device\"])\n",
    "\n",
    "in_dim = 1\n",
    "hidden_dim = 64\n",
    "out_dim = 1\n",
    "sequence_length = 24\n",
    "batch_size = 128\n",
    "\n",
    "num_epochs=15\n",
    "num_workers=11\n",
    "lr = 1e-3\n",
    "regularization=1e-6\n",
    "\n",
    "input = torch.randn(batch_size, sequence_length)\n",
    "random_data_y = torch.randn(batch_size, 1)\n",
    "print(input.shape, random_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6027, 24, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(6027, 24*3)\n",
    "sequence_length = 24\n",
    "num_features = int(X.shape[1]/sequence_length)\n",
    "# reshape X into a 3D tensor with dimensions (number of values, sequence length, number of features)\n",
    "num_sequences = X.shape[0]\n",
    "num_features = int(X.shape[1]/sequence_length)\n",
    "X_3d = X.reshape(X.shape[0], sequence_length, num_features)\n",
    "X_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11216\\2383697263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnum_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_3d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_3d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_3d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_3d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "if X.shape[-1] != 1:\n",
    "    # reshape X_train into a 3D tensor with dimensions (number of values, sequence length, number of features)\n",
    "    num_values = X.shape[0]\n",
    "    num_features = int(X.shape[1]/sequence_length)\n",
    "    X_3d = X.reshape(num_values, sequence_length, num_features)\n",
    "    X_3d = X_3d.astype(np.float32)\n",
    "    X = X_3d.copy()\n",
    "    X = torch.tensor(X)\n",
    "else:\n",
    "    X = X.unsqueeze(-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = nn.LSTMCell(in_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_out = nn.Linear(hidden_dim*sequence_length, out_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, y):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        c_t = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            h_t, c_t = self.lstm1(time_step, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        out = self.linear_out(outputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.lstm1 = nn.LSTMCell(in_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.T_A = nn.Linear(sequence_length*hidden_dim, sequence_length)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, y):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        c_t = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(y.size(0), self.hidden_dim, dtype=torch.float32)\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(time_step, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "            \n",
    "        total_ht = outputs[0]\n",
    "        for i in range(1, len(outputs)):\n",
    "            total_ht = torch.cat((total_ht, outputs[i]), 1)\n",
    "\n",
    "        beta_t =  self.relu(self.T_A(total_ht))\n",
    "        beta_t = self.softmax(beta_t)\n",
    "\n",
    "        out = torch.zeros(y.size(0), self.hidden_dim)\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "                      \n",
    "            out = out + outputs[i]*beta_t[:,i].reshape(out.size(0), 1)\n",
    "\n",
    "        out = self.linear_out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0076, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM1(1, 64, 1)\n",
    "y_pred = model(input)\n",
    "print(y_pred.shape)\n",
    "print(random_data_y.shape)\n",
    "mse_loss = nn.MSELoss()\n",
    "mse_loss(y_pred, random_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0047, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM2(1, 64, 1)\n",
    "y_pred = model(input)\n",
    "print(y_pred.shape)\n",
    "print(random_data_y.shape)\n",
    "mse_loss = nn.MSELoss()\n",
    "mse_loss(y_pred, random_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # define the linear input layer\n",
    "        self.linear_in = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # define the batch normalization layer\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # define the linear output layer\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # apply the linear input layer\n",
    "        x = self.linear_in(input)\n",
    "\n",
    "        # apply batch normalization\n",
    "        x = self.batch_norm(x.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        # apply the LSTM layer\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # check number of features\n",
    "        if input.size(2) == 1:\n",
    "            # apply the linear output layer\n",
    "            out = self.linear_out(lstm_out[:, -1, :])\n",
    "        else:\n",
    "            # apply the linear output layer\n",
    "            out = self.linear_out(lstm_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states shape: (seq_len, batch_size, hidden_size)\n",
    "        energy = torch.tanh(self.attn(hidden_states))\n",
    "        attention_weights = torch.softmax(self.v(energy), dim=0)\n",
    "        context_vector = torch.sum(attention_weights * hidden_states, dim=0)\n",
    "        return context_vector\n",
    "        \n",
    "\n",
    "class LSTMTemporalAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMTemporalAttention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # define the linear input layer\n",
    "        self.linear_in = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # define the temporal attention layer\n",
    "        self.temporal_attention = TemporalAttention(hidden_size)\n",
    "\n",
    "        # define the batch normalization layer\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        if input_size == 1:\n",
    "             # define the linear output layer\n",
    "            self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "        else:\n",
    "            # define the linear output layer\n",
    "            self.linear_out = nn.Linear(hidden_size * 2, output_size) # multiply by 2 to account for the concatenated input and attention output\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # apply the linear input layer\n",
    "        x = self.linear_in(input)\n",
    "\n",
    "        # apply batch normalization\n",
    "        x = self.batch_norm(x.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        # apply the LSTM layer\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # apply temporal attention\n",
    "        attention_out = self.temporal_attention(lstm_out.transpose(0, 1))\n",
    "\n",
    "        # check number of features\n",
    "        if input.size(2) == 1:\n",
    "            # apply the linear output layer\n",
    "            out = self.linear_out(attention_out)\n",
    "        else:\n",
    "            # concatenate attention output with input\n",
    "            out = torch.cat((attention_out.unsqueeze(1).repeat(1, x.size(1), 1), x), dim=-1)\n",
    "\n",
    "            # apply the linear output layer\n",
    "            out = self.linear_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(FCN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # define the linear input layer\n",
    "        self.linear_in = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # define the batch normalization layer\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # define the fully connected layers\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # define the linear output layer\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # apply the linear input layer\n",
    "        x = self.linear_in(input)\n",
    "\n",
    "        # apply the batch normalization layer\n",
    "        x = self.batch_norm(x.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        # reshape the input to (seq_len, batch_size, hidden_size)\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # apply the fully connected layers\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = fc_layer(x)\n",
    "\n",
    "        # check number of features\n",
    "        if input.size(2) == 1:\n",
    "            # apply the linear output layer\n",
    "            x = self.linear_out(x[-1])\n",
    "        else:\n",
    "            # apply the linear output layer\n",
    "            x = self.linear_out(x).transpose(0,1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNTemporalAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(FCNTemporalAttention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # define the linear input layer\n",
    "        self.linear_in = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # define the batch normalization layer\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # define the fully connected layers\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # define the temporal attention layer\n",
    "        self.attention = TemporalAttention(hidden_size)\n",
    "        \n",
    "        # define the linear output layer\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # define the linear layer to reshape the output\n",
    "        self.linear_reshape = nn.Linear(output_size, output_size * 25)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # apply the linear input layer\n",
    "        x = self.linear_in(input)\n",
    "\n",
    "        # apply the batch normalization layer\n",
    "        x = self.batch_norm(x.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        # reshape the input to (seq_len, batch_size, hidden_size)\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # apply the fully connected layers\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = fc_layer(x)\n",
    "\n",
    "        # apply the temporal attention layer\n",
    "        attention_out = self.attention(x)\n",
    "        \n",
    "        # apply the linear output layer\n",
    "        x = self.linear_out(attention_out)\n",
    "\n",
    "        if not input.size(2) == 1:\n",
    "            # reshape the output to (batch_size, seq_len, output_size)\n",
    "            x = self.linear_reshape(x)\n",
    "            x = x.view(x.size(0), 25, self.output_size)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 25, 31])\n",
      "torch.Size([32, 25, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = LSTM(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "input_tensor1 = torch.randn(32, 25, 1)\n",
    "output_tensor1 = model(input_tensor1)\n",
    "\n",
    "# Example usage\n",
    "model2 = LSTM(input_size=31, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "input_tensor2 = torch.randn(32, 25, 31)\n",
    "output_tensor2 = model2(input_tensor2)\n",
    "\n",
    "print(input_tensor1.shape)\n",
    "print(output_tensor1.shape)\n",
    "\n",
    "print(input_tensor2.shape)\n",
    "print(output_tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 25, 31])\n",
      "torch.Size([32, 25, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = LSTMTemporalAttention(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "input_tensor1 = torch.randn(32, 25, 1)\n",
    "output_tensor1 = model(input_tensor1)\n",
    "\n",
    "# Example usage\n",
    "model2 = LSTMTemporalAttention(input_size=31, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "input_tensor2 = torch.randn(32, 25, 31)\n",
    "output_tensor2 = model2(input_tensor2)\n",
    "\n",
    "print(input_tensor1.shape)\n",
    "print(output_tensor1.shape)\n",
    "\n",
    "print(input_tensor2.shape)\n",
    "print(output_tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(FCN1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define the temporal attention layer\n",
    "        self.attention = TemporalAttention(hidden_dim)\n",
    "\n",
    "        # define the linear input layer\n",
    "        self.linear_in = nn.Linear(input_size, hidden_size, bias=False)\n",
    "\n",
    "        # define the batch normalization layer\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # define the fully connected layers\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size, bias=False) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # define the linear output layer\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward (self,input):\n",
    "        out = self.linear_in(input)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        \n",
    "        # apply the fully connected layers\n",
    "        for fc_layer in self.fc_layers:\n",
    "            out = fc_layer(out)\n",
    "            out = self.sigmoid(out)            \n",
    "\n",
    "        out = self.linear_out(out)\n",
    "\n",
    "        if input.size(2) == 1:\n",
    "            out = out.transpose(0, 1)[-1]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 25, 31])\n",
      "torch.Size([32, 25, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = FCN1(1, 64, 2, 1)\n",
    "\n",
    "input_tensor1 = torch.randn(32, 25, 1)\n",
    "output_tensor1 = model(input_tensor1)\n",
    "\n",
    "# Example usage\n",
    "model2 = FCN1(31, 64, 2, 1)\n",
    "\n",
    "input_tensor2 = torch.randn(32, 25, 31)\n",
    "output_tensor2 = model2(input_tensor2)\n",
    "\n",
    "print(input_tensor1.shape)\n",
    "print(output_tensor1.shape)\n",
    "\n",
    "print(input_tensor2.shape)\n",
    "print(output_tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 25, 31])\n",
      "torch.Size([32, 25, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model = FCNTemporalAttention(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "input_tensor1 = torch.randn(32, 25, 1)\n",
    "output_tensor1 = model(input_tensor1)\n",
    "\n",
    "# Example usage\n",
    "model2 = FCNTemporalAttention(input_size=31, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "input_tensor2 = torch.randn(32, 25, 31)\n",
    "output_tensor2 = model2(input_tensor2)\n",
    "\n",
    "print(input_tensor1.shape)\n",
    "print(output_tensor1.shape)\n",
    "\n",
    "print(input_tensor2.shape)\n",
    "print(output_tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMTest(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMTest, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # define the linear input layer\n",
    "        self.linear_in = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # define the batch normalization layer\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # define the linear output layer\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply the linear input layer\n",
    "        x = self.linear_in(x)\n",
    "\n",
    "        # apply batch normalization\n",
    "        x = self.batch_norm(x.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        # apply the LSTM layer\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # apply the linear output layer\n",
    "        out = self.linear_out(lstm_out[:, -1, :])\n",
    "\n",
    "        # squeeze the output tensor to shape [batch_size]\n",
    "        out = out.squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25, 3])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "model = LSTMTest(input_size=3, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "input_tensor1 = torch.randn(32, 25, 3)\n",
    "output_tensor1 = model(input_tensor1)\n",
    "\n",
    "print(input_tensor1.shape)\n",
    "print(output_tensor1.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab06ce908a8abf9762b166587a79a7b3fe0760e63d13e53395d21ef1a2a21042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
