{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from filelock import FileLock\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import new_models\n",
    "from config import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/\"\n",
    "target_variable = \"Q_Kalltveit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(file_path):\n",
    "    \"\"\"Loads a pandas DataFrame from a CSV or XLSX file.\"\"\"\n",
    "    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "        # get file extension\n",
    "        file_ext = os.path.splitext(file_path)[1]\n",
    "\n",
    "        # check if file is .csv or .xlsx\n",
    "        if file_ext == \".csv\":\n",
    "            data = pd.read_csv(file_path, index_col='Datetime')\n",
    "        elif file_ext == \".xlsx\":\n",
    "            data = pd.read_excel(file_path)\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    # Construct file path using os.path.join\n",
    "\n",
    "    raw_data_path = os.path.join(data_dir, \"raw_data\", \"cascaded_use_case_data.xlsx\")\n",
    "    \n",
    "    if os.path.isfile(raw_data_path):\n",
    "        return load_data_from_file(raw_data_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Raw data file does not exist at path: {}\".format(raw_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_matrix(window_size, vars_to_lag=None, pca=False, mi=False): #TODO: Fix decimal (five values in dataframe four in X, y)\n",
    "    \"\"\"\n",
    "    Create a lagged matrix from time series data.\n",
    "    Args:\n",
    "    - window_size: number of lags to include.\n",
    "    - vars_to_lag: list of variable names to include in the lagged matrix.\n",
    "    If None, all variables except the target variable are included.\n",
    "    Returns:\n",
    "    - X: tensor array of shape (n_samples, window_size, ).\n",
    "    - y: tensor array of shape (n_samples,).\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct file path using os.path.join\n",
    "    path = os.path.join(\n",
    "        data_dir, \n",
    "        \"clean_data\", \n",
    "        \"multivariate\", \n",
    "        target_variable, \n",
    "        f\"{window_size}_lag_\" + (\"_\".join(vars_to_lag) if vars_to_lag else \"\") + \".csv\")\n",
    "        \n",
    "    if os.path.isfile(path): \n",
    "        lagged_df = load_data_from_file(path)\n",
    "    else:\n",
    "        data = get_raw_data()\n",
    "\n",
    "        if vars_to_lag:\n",
    "            lagged_df = data[[target_variable] + vars_to_lag].copy()\n",
    "            # create a lagged matrix of target and variables\n",
    "            for i in range(1, window_size+1):\n",
    "                for var in vars_to_lag:\n",
    "                    lagged_df.loc[:, f'{var}_{i}'] = lagged_df[var].shift(i)\n",
    "        else:\n",
    "            lagged_df = data[[target_variable]].copy()\n",
    "            # create a lagged matrix of target\n",
    "            for i in range(1, window_size+1):\n",
    "                lagged_df.loc[:, f'{target_variable}_{i}'] = lagged_df[target_variable].shift(i)\n",
    "\n",
    "        # set datetime to index\n",
    "        lagged_df.index = data['Datetime']\n",
    "\n",
    "        # remove rows with NaN values\n",
    "        lagged_df.dropna(inplace=True)\n",
    "\n",
    "        # save lagged matrix\n",
    "        lagged_df.to_csv(path, index=True)\n",
    "    \n",
    "    # separate the target variable from the input variables\n",
    "    X = lagged_df.drop(columns=[f'{target_variable}'], axis=1)\n",
    "    y = lagged_df[f'{target_variable}'] # TODO: methods such as Granger causality or structural equation modeling to determine whether the lagged values of \"Nedbør Nilsebu\" and \"Q_Lyngsaana\" are predictive of future values of \"Q_Kalltveit\".\n",
    "\n",
    "    X = torch.tensor(np.array(X)).float()\n",
    "    y = torch.tensor(np.array(y)).float()\n",
    "    if not vars_to_lag:\n",
    "        # reshape X into a 3D tensor with dimensions \n",
    "        # (number of sequences, sequence length, 1) if univariate\n",
    "        X = X.unsqueeze(-1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_size=0.7, val_size=0.2, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array-like): The input data.\n",
    "        y (array-like): The target data.\n",
    "        train_size (float): The proportion of the dataset to use for training.\n",
    "        val_size (float): The proportion of the dataset to use for validation.\n",
    "        test_size (float): The proportion of the dataset to use for testing.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (X_train, y_train, X_val, y_val, X_test, y_test) containing the\n",
    "        training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Check that the sizes add up to 1.0\n",
    "    if round(train_size + val_size + test_size, 2) != 1.0:\n",
    "        raise ValueError(\"Train, validation, and test sizes must add up to 1.0\")\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, train_size=train_size, shuffle=True)\n",
    "\n",
    "    # Compute the validation size relative to the remaining data after the train split\n",
    "    val_size_ratio = test_size / (val_size + test_size)\n",
    "    \n",
    "    # Split the remaining data into training and validation sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test,\n",
    "                                                    test_size=val_size_ratio,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import mean_absolute_error\n",
    "\n",
    "def fit(model, loss_function, optimizer, data_loader, num_epochs, mode, use_amp=False):\n",
    "\thistory = {\"train\": {\"loss\": [], \"mae\": []}, \"val\": {\"loss\": [], \"mae\": []}}\n",
    "\tscaler = torch.cuda.amp.GradScaler(enabled=use_amp) # Mixed-precision support for compatible GPUs\n",
    "\tprint(\"\\nTraining the model:\")\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint(\"\\nEpoch\", epoch+1)\n",
    "\t\tif epoch < num_epochs - 1:\n",
    "\t\t\tkeys = [\"train\", \"val\"]\n",
    "\t\telse:\n",
    "\t\t\tkeys = [\"train\", \"val\", \"test\"]\n",
    "\t\tfor key in keys:\n",
    "\t\t\tdataset_size = 0\n",
    "\t\t\tdataset_loss = 0.0\n",
    "\t\t\tif key == \"train\":\n",
    "\t\t\t\tmodel.train()\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.eval()\n",
    "\t\t\tfor X_batch, y_batch in tqdm(data_loader[key]):\n",
    "\t\t\t\tX_batch, y_batch = X_batch.to(mode[\"device\"]), y_batch.to(mode[\"device\"])\n",
    "\t\t\t\twith torch.set_grad_enabled(mode=(key==\"train\")): # Autograd activated only during training\n",
    "\t\t\t\t\twith torch.cuda.amp.autocast(enabled=use_amp): # Mixed-precision support for compatible GPUs\n",
    "\t\t\t\t\t\tbatch_output = model(X_batch.float())\n",
    "\t\t\t\t\t\tbatch_loss = loss_function(batch_output, y_batch)\n",
    "\t\t\t\t\tif key == \"train\":\n",
    "\t\t\t\t\t\tscaler.scale(batch_loss).backward()\n",
    "\t\t\t\t\t\tscaler.step(optimizer) \t\n",
    "\t\t\t\t\t\tscaler.update()\n",
    "\t\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tdataset_size += y_batch.shape[0]\n",
    "\t\t\t\tdataset_loss += y_batch.shape[0] * batch_loss.item()\n",
    "\t\t\tdataset_loss /= dataset_size\n",
    "\t\t\tif key in [\"train\", \"val\"]:\n",
    "\t\t\t\thistory[key][\"loss\"].append(dataset_loss)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"\\nEvaluating the model:\")\n",
    "\t\t\tprint(key, \"loss:\", dataset_loss)\n",
    "\treturn history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, sequence_length, batch_size, shuffle):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch DataLoader from input data X and target data y.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): The input data.\n",
    "        y (ndarray): The target data.\n",
    "        sequence_length (int): The length of each sequence in the input data.\n",
    "        batch_size (int): The batch size to use for the DataLoader.\n",
    "        shuffle (bool): Whether to shuffle the data before creating the DataLoader.\n",
    "    \n",
    "    Returns:\n",
    "        A PyTorch DataLoader object.\n",
    "    \"\"\"\n",
    "    if X.shape[-1] != 1:\n",
    "        # reshape X_train into a 3D tensor with dimensions (number of sequences, sequence length, number of features)\n",
    "        num_sequences = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        X_3d = np.zeros((num_sequences, sequence_length, num_features))\n",
    "        for i in range(sequence_length, num_sequences):\n",
    "            X_3d[i] = X[i-sequence_length:i, :]\n",
    "        X_3d = X_3d.astype(np.float32)\n",
    "        X = X_3d.copy()\n",
    "        X = torch.tensor(X)\n",
    "\n",
    "    # create a PyTorch dataset and dataloader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "\n",
    "    use_GPU = torch.cuda.is_available()\n",
    "    if use_GPU:\n",
    "        mode = {\"name\": \"cuda\", \"device\": torch.device(\"cuda\")}\n",
    "    else:\n",
    "        mode = {\"name\": \"cpu\", \"device\": torch.device(\"cpu\")}\n",
    "\n",
    "    # Define hyperparameters\n",
    "    train_size = 0.6\n",
    "    val_size = 0.2\n",
    "    test_size = 0.2\n",
    "\n",
    "    sequence_length = 25\n",
    "    batch_size = 256\n",
    "    num_epochs = 20\n",
    "    lr = 0.00001\n",
    "    weight_decay = 0\n",
    "\n",
    "    vars = None #[\"Nedbør Nilsebu\", \"Q_Lyngsaana\"]\n",
    "    \n",
    "    ########################################################################################################################\n",
    "    ld = load_data(data_dir = data_dir, target_variable = 'Q_Kalltveit') # god\n",
    "    \n",
    "    X, y = ld.create_lagged_matrix(window_size=sequence_length, vars_to_lag=vars)\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = ld.split_data(X, y)\n",
    "    \n",
    "    train_dataloader = ld.create_dataloader(X_train, y_train, sequence_length, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = ld.create_dataloader(X_val, y_val, sequence_length, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = ld.create_dataloader(X_test, y_test, sequence_length, batch_size=batch_size, shuffle=False)\n",
    "    ########################################################################################################################\n",
    "\n",
    "\n",
    "    for i, j in test_dataloader:\n",
    "        print(i.shape)\n",
    "        break\n",
    "    \n",
    "    # Model inputs\n",
    "    input_size = X_train.shape[-1]\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "\n",
    "    net = new_models.LSTM(input_size,\n",
    "                    hidden_size,\n",
    "                    num_layers,\n",
    "                    output_size,\n",
    "                    )\n",
    "    \n",
    "    data_loader = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"val\": val_dataloader,\n",
    "    \"test\": test_dataloader,\n",
    "    }\n",
    "    \n",
    "    net.to(mode[\"device\"])\n",
    "\n",
    "    for i, j in test_dataloader:\n",
    "        print(i.shape)\n",
    "        break\n",
    "\n",
    "    loss_function = nn.MSELoss().to(mode[\"device\"])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Define your learning rate scheduler\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "                                           \n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=False) # Mixed-precision support for compatible GPUs\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\nEpoch\", epoch+1)\n",
    "        if epoch < num_epochs - 1:\n",
    "            keys = [\"train\", \"val\"]\n",
    "        else:\n",
    "            keys = [\"train\", \"val\", \"test\"]\n",
    "        for key in keys:\n",
    "            dataset_size = 0\n",
    "            dataset_loss = 0.0\n",
    "            if key == \"train\":\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "            for X_batch, y_batch in tqdm(data_loader[key]):\n",
    "                X_batch, y_batch = X_batch.to(mode[\"device\"]), y_batch.to(mode[\"device\"])\n",
    "                with torch.set_grad_enabled(mode=(key==\"train\")): # Autograd activated only during training\n",
    "                    with torch.cuda.amp.autocast(enabled=False): # Mixed-precision support for compatible GPUs\n",
    "                        batch_output = net(X_batch.float())\n",
    "                        batch_loss = loss_function(batch_output, y_batch)\n",
    "                    if key == \"train\":\n",
    "                        scaler.scale(batch_loss).backward()\n",
    "                        scaler.step(optimizer) \t\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                dataset_size += y_batch.shape[0]\n",
    "                dataset_loss += y_batch.shape[0] * batch_loss.item()\n",
    "\n",
    "            dataset_loss /= dataset_size\n",
    "\n",
    "            # Report results to Ray Tune\n",
    "            if key == \"train\":\n",
    "                pass\n",
    "            elif key == \"val\":\n",
    "                # Update learning rate\n",
    "                lr_scheduler.step(metrics=dataset_loss)\n",
    "            else:\n",
    "                print(\"\\nEvaluating the model:\")\n",
    "                print(key, \"loss:\", dataset_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (25,25) into shape (25,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16632\\915063938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16632\\210887714.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mval_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Code\\hydro-ml\\config\\load_data.py\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[1;34m(self, X, y, sequence_length, batch_size, shuffle)\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mX_3d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mX_3d\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mX_3d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_3d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_3d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (25,25) into shape (25,1)"
     ]
    }
   ],
   "source": [
    "train_model(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
